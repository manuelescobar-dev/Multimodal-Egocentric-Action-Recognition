{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started for the AML 2023/2024 Egocentric Vision Project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EgovisionPolito/aml23-ego/blob/master/colab_runner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EPIC-Kitchens-55 dataset\n",
        "\n",
        "**READ carefully!**\n",
        "\n",
        "To develop the project, you need to download the RGB frames for a subset of EPIC-Kitchens-55 (participants P08, P01 and P22) from [here](https://drive.google.com/drive/u/1/folders/1dJOtZ07WovP3YSCRAnU0E4gsfqDzpMVo). \n",
        "\n",
        "You also need to the pretrained checkpoints for each domain from [here](https://politoit-my.sharepoint.com/:f:/g/personal/simone_peirone_polito_it/ErdsZhvmR65Lun5_5O0-l5sBTPjCCZZq2f700Tj_CNzjTQ?e=L1yflf).\n",
        "\n",
        "Add the Google Drive directory containing the dataset to your Google Drive or upload the dataset on your Google Drive to access it from Google Colab.\n",
        "\n",
        "**NOTE**: As the dataset is quite heavy, we stronly suggest you to implement and test all your code on one for the three dataset. Then, once you are sure everything works, repeat the experiments on the remaining two datasets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Features extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "88YghJyXhbfS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-09 21:05:38 LOG INFO Feature Extraction\n",
            "2024-02-09 21:05:38 LOG INFO Running with parameters: \n",
            "  action: save\n",
            "  name: features1\n",
            "  modality: ['RGB']\n",
            "  total_batch: 128\n",
            "  batch_size: 32\n",
            "  gpus: None\n",
            "  wandb_name: None\n",
            "  resume_from: ./saved_models/I3D_SourceOnlyD1\n",
            "  logname: save_D1-D1.log\n",
            "  models_dir: saved_models/features1/Feb09_21-05-38\n",
            "  train:\n",
            "    num_iter: 5000\n",
            "    lr_steps: 3000\n",
            "    eval_freq: 50\n",
            "    num_clips: 1\n",
            "    dense_sampling:\n",
            "      RGB: True\n",
            "    num_frames_per_clip:\n",
            "      RGB: 16\n",
            "  test:\n",
            "    num_clips: 5\n",
            "    dense_sampling:\n",
            "      RGB: True\n",
            "    num_frames_per_clip:\n",
            "      RGB: 16\n",
            "  dataset:\n",
            "    annotations_path: train_val\n",
            "    shift: D1-D1\n",
            "    workers: 4\n",
            "    stride: 2\n",
            "    resolution: 224\n",
            "    RGB:\n",
            "      data_path: ../EK\n",
            "      tmpl: img_{:010d}.jpg\n",
            "      features_name: test_feat_kinetics\n",
            "    Event:\n",
            "      rgb4e: 6\n",
            "  models:\n",
            "    RGB:\n",
            "      model: I3D\n",
            "      normalize: False\n",
            "      kwargs:\n",
            "      lr_steps: 3000\n",
            "      lr: 0.01\n",
            "      sgd_momentum: 0.9\n",
            "      weight_decay: 1e-07\n",
            "      dropout: 0.5\n",
            "      resolution: 224\n",
            "      weight_i3d_rgb: ./pretrained_i3d/rgb_imagenet.pt\n",
            "  split: test\n",
            "  save:\n",
            "    num_clips: 5\n",
            "    dense_sampling:\n",
            "      RGB: True\n",
            "    num_frames_per_clip:\n",
            "      RGB: 16\n",
            "  config: configs/I3D_save_feat.yaml\n",
            "  experiment_dir: features1/Feb09_21-05-38\n",
            "  log_dir: TEST_RESULTS/features1\n",
            "  logfile: TEST_RESULTS/features1/save_D1-D1.log\n",
            "2024-02-09 21:05:38 LOG INFO Instantiating models per modality\n",
            "2024-02-09 21:05:38 LOG INFO I3D Net\tModality: RGB\n",
            "2024-02-09 21:05:38 LOG INFO Loading Kinetics weights I3D\n",
            "2024-02-09 21:05:38 LOG INFO  * Skipping Logits weight for 'logits.conv3d.weight'\n",
            "2024-02-09 21:05:38 LOG INFO  * Skipping Logits weight for 'logits.conv3d.bias'\n",
            "2024-02-09 21:05:38 LOG INFO Restoring action-classifier for modality RGB from saved_models/I3D_SourceOnlyD1/Oct25_22-38-50/action-classifier_RGB_9.pth\n",
            "2024-02-09 21:05:39 LOG INFO RGB-Model for action-classifier restored at iter 4850.0\n",
            "Best accuracy on val: 59.54 at iter 4000.0\n",
            "Last accuracy on val: 58.85\n",
            "Last loss: 0.00\n",
            "2024-02-09 21:05:39 LOG INFO Dataloader for D1-test with 435 samples generated\n",
            "2024-02-09 21:05:42 LOG ERROR Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/manuelescobar/Files/2023-2/AML/MEAR/Multimodal-Egocentric-Action-Recognition/manuel/src/save_feat.py\", line 159, in <module>\n",
            "    main()\n",
            "  File \"/Users/manuelescobar/Files/2023-2/AML/MEAR/Multimodal-Egocentric-Action-Recognition/manuel/src/save_feat.py\", line 70, in main\n",
            "    save_feat(action_classifier, loader, device, action_classifier.current_iter, num_classes)\n",
            "  File \"/Users/manuelescobar/Files/2023-2/AML/MEAR/Multimodal-Egocentric-Action-Recognition/manuel/src/save_feat.py\", line 94, in save_feat\n",
            "    for i_val, (data, label, video_name, uid) in enumerate(loader):\n",
            "  File \"/Users/manuelescobar/miniconda3/envs/egovision/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/Users/manuelescobar/miniconda3/envs/egovision/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1376, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/Users/manuelescobar/miniconda3/envs/egovision/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1402, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/Users/manuelescobar/miniconda3/envs/egovision/lib/python3.10/site-packages/torch/_utils.py\", line 461, in reraise\n",
            "    raise exception\n",
            "NotImplementedError: Caught NotImplementedError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/Users/manuelescobar/miniconda3/envs/egovision/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/Users/manuelescobar/miniconda3/envs/egovision/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/Users/manuelescobar/miniconda3/envs/egovision/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/Users/manuelescobar/Files/2023-2/AML/MEAR/Multimodal-Egocentric-Action-Recognition/manuel/src/utils/loaders.py\", line 119, in __getitem__\n",
            "    segment_indices[modality] = self._get_val_indices(record, modality)\n",
            "  File \"/Users/manuelescobar/Files/2023-2/AML/MEAR/Multimodal-Egocentric-Action-Recognition/manuel/src/utils/loaders.py\", line 88, in _get_val_indices\n",
            "    raise NotImplementedError(\"You should implement _get_val_indices\")\n",
            "NotImplementedError: You should implement _get_val_indices\n",
            "\n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b\"\\npython save_feat.py name=features1 \\\\\\n  config=configs/I3D_save_feat.yaml \\\\\\n  dataset.shift=D1-D1 \\\\\\n  dataset.RGB.data_path=../EK\\n\\n# If everything is working, you should expect an error message telling you to implement the '_get_val_indices' method in the dataset class.\\n# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features.\\n\"' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython save_feat.py name=features1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  config=configs/I3D_save_feat.yaml \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  dataset.shift=D1-D1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  dataset.RGB.data_path=../EK\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# If everything is working, you should expect an error message telling you to implement the \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_get_val_indices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m method in the dataset class.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/egovision/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m~/miniconda3/envs/egovision/lib/python3.10/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/egovision/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"\\npython save_feat.py name=features1 \\\\\\n  config=configs/I3D_save_feat.yaml \\\\\\n  dataset.shift=D1-D1 \\\\\\n  dataset.RGB.data_path=../EK\\n\\n# If everything is working, you should expect an error message telling you to implement the '_get_val_indices' method in the dataset class.\\n# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features.\\n\"' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "python save_feat.py name=features1 \\\n",
        "  config=configs/I3D_save_feat.yaml \\\n",
        "  dataset.shift=D1-D1 \\\n",
        "  dataset.RGB.data_path=../EK\n",
        "\n",
        "# If everything is working, you should expect an error message telling you to implement the '_get_val_indices' method in the dataset class.\n",
        "# Once you have implemented it, you should run the script for the train and test split of the dataset to extract the features."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMA44pwS84HIKtaEclSmH2W",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "aml22",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "2fc1f0eeae38a5df67b0f713e03196095ce1bfa55aa551576e8e58c2ba904c5a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
