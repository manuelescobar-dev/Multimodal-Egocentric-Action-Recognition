{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=\"data\"\n",
    "ANNOTATIONS_DIR = \"train_val\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR,\"raw\")\n",
    "TEMP_DATA_DIR = os.path.join(DATA_DIR,\"temp_a\")\n",
    "FINAL_DATA_DIR = os.path.join(DATA_DIR,\"final_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        df = pd.read_pickle(f)\n",
    "    return df\n",
    "\n",
    "def save_data(df, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pd.to_pickle(df, f)\n",
    "        \n",
    "def save_step(train_data, test_data, step):\n",
    "    save_data(train_data, os.path.join(TEMP_DATA_DIR, f\"train_{step}.pkl\"))\n",
    "    save_data(test_data, os.path.join(TEMP_DATA_DIR, f\"test_{step}.pkl\"))\n",
    "\n",
    "        \n",
    "def dataset_info(data):\n",
    "    print(\"Dataset shape: \", data.shape)\n",
    "    print(\"Dataset columns: \", data.columns)\n",
    "    # Number of classes\n",
    "    print(\"Number of classes: \", len(data['description'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = load_data('train_val/train.pkl')\n",
    "test_split = load_data('train_val/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (527, 4)\n",
      "Dataset columns:  Index(['index', 'file', 'description', 'labels'], dtype='object')\n",
      "Number of classes:  22\n",
      "Dataset shape:  (59, 4)\n",
      "Dataset columns:  Index(['index', 'file', 'description', 'labels'], dtype='object')\n",
      "Number of classes:  20\n"
     ]
    }
   ],
   "source": [
    "dataset_info(train_split)\n",
    "dataset_info(test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP=\"remove_duplicates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes ={'Slice a potato', 'Spread jelly on a bread slice', 'Load dishwasher: 3 each large/small plates, bowls, mugs, glasses, sets of utensils', 'Unload dishwasher: 3 each large/small plates, bowls, mugs, glasses, sets of utensils', 'Clear cutting board', 'Get items from cabinets: 3 each large/small plates, bowls, mugs, glasses, sets of utensils', 'Spread almond butter on a bread slice', 'Clean a plate with a sponge', 'Slice a cucumber', 'Clean a pan with a sponge', 'Slice bread', 'Clean a plate with a towel', 'Pour water from a pitcher into a glass', 'Peel a cucumber', 'Set table: 3 each large/small plates, bowls, mugs, glasses, sets of utensils', 'Open/close a jar of almond butter', 'Peel a potato', 'Get/replace items from refrigerator/cabinets/drawers', 'Stack on table: 3 each large/small plates, bowls', 'Clean a pan with a towel'}\n",
    "classes_map = {c: i for i, c in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_synonyms(description):\n",
    "    if description==\"Get items from refrigerator/cabinets/drawers\":\n",
    "        return \"Get/replace items from refrigerator/cabinets/drawers\"\n",
    "    elif description==\"Open a jar of almond butter\":\n",
    "        return \"Open/close a jar of almond butter\"\n",
    "    else:\n",
    "        return description\n",
    "    \n",
    "def filter_dataset(data, name, step=STEP):\n",
    "    filtered_dataset = data.copy()\n",
    "    classes = filtered_dataset['description'].unique()\n",
    "    print(f\"{len(classes)} classes found.\")\n",
    "    filtered_dataset['description'] = filtered_dataset['description'].apply(remove_synonyms)\n",
    "    classes = filtered_dataset['description'].unique()\n",
    "    print(f\"{len(classes)} classes after removing synonyms.\")\n",
    "    filtered_dataset['label'] = filtered_dataset['description'].apply(lambda x: classes_map[x])\n",
    "    # Drop column labels\n",
    "    filtered_dataset = filtered_dataset.drop(columns=['labels'])\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 classes found.\n",
      "20 classes after removing synonyms.\n",
      "20 classes found.\n",
      "19 classes after removing synonyms.\n"
     ]
    }
   ],
   "source": [
    "train_split_filtered = filter_dataset(train_split, \"train\")\n",
    "test_split_filtered = filter_dataset(test_split, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (527, 4)\n",
      "Dataset columns:  Index(['index', 'file', 'description', 'label'], dtype='object')\n",
      "Number of classes:  20\n",
      "Dataset shape:  (59, 4)\n",
      "Dataset columns:  Index(['index', 'file', 'description', 'label'], dtype='object')\n",
      "Number of classes:  19\n"
     ]
    }
   ],
   "source": [
    "dataset_info(train_split_filtered)\n",
    "dataset_info(test_split_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_step(train_split_filtered, test_split_filtered, STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP=\"merge\"\n",
    "train_data = load_data(os.path.join(TEMP_DATA_DIR, \"train_remove_duplicates.pkl\"))\n",
    "test_data = load_data(os.path.join(TEMP_DATA_DIR, \"test_remove_duplicates.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(mode):\n",
    "    if mode==\"train\":\n",
    "        data = load_data(os.path.join(TEMP_DATA_DIR, \"train_remove_duplicates.pkl\"))\n",
    "    elif mode==\"test\":\n",
    "        data = load_data(os.path.join(TEMP_DATA_DIR, \"test_remove_duplicates.pkl\"))\n",
    "    rows = []\n",
    "    for tup in data.iterrows():\n",
    "        row = tup[1].copy()\n",
    "        file = row[\"file\"]\n",
    "        idx = row[\"index\"]\n",
    "        emg = load_data(os.path.join(RAW_DATA_DIR, file))\n",
    "        emg = emg.iloc[idx].copy()\n",
    "        emg[\"subject\"]=file.split('.')[0]\n",
    "        emg[\"label\"]=row[\"label\"]\n",
    "        emg[\"description\"]=row[\"description\"]\n",
    "        rows.append(emg)\n",
    "    merged = pd.DataFrame(rows)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = merge(\"train\")\n",
    "test_data = merge(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (527, 9)\n",
      "Dataset columns:  Index(['description', 'start', 'stop', 'myo_left_timestamps',\n",
      "       'myo_left_readings', 'myo_right_timestamps', 'myo_right_readings',\n",
      "       'subject', 'label'],\n",
      "      dtype='object')\n",
      "Number of classes:  20\n",
      "Dataset shape:  (59, 9)\n",
      "Dataset columns:  Index(['description', 'start', 'stop', 'myo_left_timestamps',\n",
      "       'myo_left_readings', 'myo_right_timestamps', 'myo_right_readings',\n",
      "       'subject', 'label'],\n",
      "      dtype='object')\n",
      "Number of classes:  19\n"
     ]
    }
   ],
   "source": [
    "dataset_info(train_data)\n",
    "dataset_info(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_step(train_data, test_data, STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = \"emg\"\n",
    "train_data = load_data(os.path.join(TEMP_DATA_DIR, f\"train_merge.pkl\"))\n",
    "test_data = load_data(os.path.join(TEMP_DATA_DIR, f\"test_merge.pkl\"))\n",
    "FS = 160  # Sampling frequency\n",
    "CUTOFF = 5  # Cutoff frequency\n",
    "NUM_CHANNELS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectify_signal(data):\n",
    "    return np.abs(data)\n",
    "\n",
    "def filter_signal(data):\n",
    "    for i in range(NUM_CHANNELS):\n",
    "        data[:,i] = low_pass_filter(data[:,i])\n",
    "    return data\n",
    "\n",
    "def low_pass_filter(data, order=5):\n",
    "    nyquist = 0.5 * FS\n",
    "    normal_cutoff = CUTOFF / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return filtfilt(b, a, data, padlen=5)\n",
    "\n",
    "def normalization(data, mean, std):\n",
    "    \"\"\"Normalize with mean and std\n",
    "    data: (n_samples, n_channels)\n",
    "    mean and std: (n_channels,)\n",
    "    \"\"\"\n",
    "    return (data - mean) / std\n",
    "\n",
    "def mean_std(data):\n",
    "    sides=[\"left\", \"right\"]\n",
    "    means=[]\n",
    "    stds=[]\n",
    "    for s in sides:\n",
    "        normalized_data = data.copy()\n",
    "        normalized_data[f\"myo_{s}_mean\"] = normalized_data[f\"myo_{s}_readings\"].apply(lambda x: np.mean(x, axis=0))\n",
    "        normalized_data[f\"myo_{s}_std\"] = normalized_data[f\"myo_{s}_readings\"].apply(lambda x: np.std(x, axis=0))\n",
    "        means.append(np.mean(normalized_data[f\"myo_{s}_mean\"].to_list(), axis=0))\n",
    "        stds.append(np.mean(normalized_data[f\"myo_{s}_std\"].to_list(), axis=0))\n",
    "    return (means[0], stds[0]), (means[1], stds[1])\n",
    "\n",
    "def preprocess(train_data, test_data, normalize=False):\n",
    "    # Preprocess train data\n",
    "    train = train_data.copy()\n",
    "    steps=[rectify_signal, filter_signal]\n",
    "    sides=[\"left\", \"right\"]\n",
    "    for side in sides:\n",
    "        for step in steps:\n",
    "            train[f\"myo_{side}_readings\"] = train[f\"myo_{side}_readings\"].apply(step)\n",
    "    if normalize:\n",
    "        (left_mean, left_std), (right_mean, right_std) = mean_std(train)\n",
    "        print(left_mean, left_std, right_mean, right_std)\n",
    "        train[f\"myo_left_readings\"] = train[f\"myo_left_readings\"].apply(normalization, args=(left_mean, left_std))\n",
    "        train[f\"myo_right_readings\"] = train[f\"myo_right_readings\"].apply(normalization, args=(right_mean, right_std))\n",
    "    # Preprocess test data\n",
    "    test = test_data.copy()\n",
    "    for side in sides:\n",
    "        for step in steps:\n",
    "            test[f\"myo_{side}_readings\"] = test[f\"myo_{side}_readings\"].apply(step)\n",
    "    if normalize:\n",
    "        test[f\"myo_left_readings\"] = test[f\"myo_left_readings\"].apply(normalization, args=(left_mean, left_std))\n",
    "        test[f\"myo_right_readings\"] = test[f\"myo_right_readings\"].apply(normalization, args=(right_mean, right_std))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.75624219 13.06355075 14.42117728 15.51845746 16.66506348 12.8855857\n",
      " 10.57188832 10.54096876] [7.42496851 6.0592204  6.56498921 7.43746219 8.41245864 7.11516333\n",
      " 5.51764957 5.98278444] [13.54149584 16.74649575 19.52858889 16.95801181 12.46527825  9.95593723\n",
      " 12.8419363  10.88651791] [7.31494636 9.73468516 9.50411395 8.2842876  6.1577131  5.2747223\n",
      " 7.71895575 6.48097835]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train, preprocessed_test = preprocess(train_data, test_data, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (527, 9)\n",
      "Dataset columns:  Index(['description', 'start', 'stop', 'myo_left_timestamps',\n",
      "       'myo_left_readings', 'myo_right_timestamps', 'myo_right_readings',\n",
      "       'subject', 'label'],\n",
      "      dtype='object')\n",
      "Number of classes:  20\n",
      "Dataset shape:  (59, 9)\n",
      "Dataset columns:  Index(['description', 'start', 'stop', 'myo_left_timestamps',\n",
      "       'myo_left_readings', 'myo_right_timestamps', 'myo_right_readings',\n",
      "       'subject', 'label'],\n",
      "      dtype='object')\n",
      "Number of classes:  19\n"
     ]
    }
   ],
   "source": [
    "dataset_info(preprocessed_train)\n",
    "dataset_info(preprocessed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    myo_left_readings  \\\n",
      "40  [[-1.0446161729358687, -0.3405637377935135, 1....   \n",
      "10  [[-1.5833389963599647, -2.155978803913611, -0....   \n",
      "6   [[-0.9099354670798447, -0.5056014710771587, -0...   \n",
      "24  [[-1.3139775846479167, -0.6706392043608039, -2...   \n",
      "6   [[-0.9099354670798447, -1.3307901374953848, -2...   \n",
      "\n",
      "                                   myo_right_readings  \n",
      "40  [[-1.7145027755423212, -1.925742378200468, 0.7...  \n",
      "10  [[-1.5777963732581994, -0.8984877896617861, -1...  \n",
      "6   [[1.156331672424237, 0.2314922577307637, -0.58...  \n",
      "24  [[-1.1676771664058339, -0.17940957768470897, 4...  \n",
      "6   [[-0.8942643618375903, -0.48758595424631346, 2...  \n",
      "                                    myo_left_readings  \\\n",
      "4   [[-1.8527004080720129, 4.775605993999488, 1.91...   \n",
      "34  [[-1.4486582905039407, 2.7951531945957453, -2....   \n",
      "48  [[-1.8527004080720129, -1.9909410706299657, -2...   \n",
      "4   [[-0.3712126436557488, -1.6608656040626752, -2...   \n",
      "62  [[-1.7180197022159887, -1.6608656040626752, -2...   \n",
      "\n",
      "                                   myo_right_readings  \n",
      "4   [[-1.1676771664058339, 0.026041340023027375, -...  \n",
      "34  [[-1.4410899709740776, -1.7202914604927315, -0...  \n",
      "48  [[-1.3043835686899559, 2.5941778113697316, 0.4...  \n",
      "4   [[-1.5777963732581994, -1.5148405427849951, -0...  \n",
      "62  [[-0.6208515572693467, -1.7202914604927315, -2...  \n"
     ]
    }
   ],
   "source": [
    "print(preprocessed_train[[\"myo_left_readings\",\"myo_right_readings\"]].head())\n",
    "print(preprocessed_test[[\"myo_left_readings\",\"myo_right_readings\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_step(preprocessed_train, preprocessed_test, STEP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
